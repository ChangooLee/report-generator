import asyncio
import json
import logging
import os
import tempfile
from enum import Enum
from typing import Dict, Any, Optional, List, AsyncIterator
import httpx
from dotenv import load_dotenv

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì… ì—´ê±°í˜•"""
    LLM = "llm"
    QWEN = "qwen"

class OpenRouterClient:
    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ ì¬ë¡œë“œ ì‹œë„ - override=Trueë¡œ ê°•ì œ ê°±ì‹ 
        load_dotenv(override=True)
        
        self.base_url = os.getenv('LLM_API_BASE_URL') or os.getenv('VLLM_API_BASE_URL', 'https://openrouter.ai/api/v1')
        self.api_key = os.getenv('LLM_API_KEY') or os.getenv('VLLM_API_KEY') or os.getenv('CLAUDE_API_KEY')
        self.qwen_model = os.getenv('VLLM_MODEL_NAME', 'Qwen/Qwen2.5-Coder-32B-Instruct')
        self.llm_model = os.getenv('LLM_NAME', 'deepseek/deepseek-chat-v3-0324')
        
        if not self.api_key:
            raise ValueError("LLM_API_KEY ë˜ëŠ” VLLM_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info("ğŸ”‘ OpenRouter API í‚¤ ë¡œë“œ: âœ… ì„±ê³µ")
        logger.info(f"ğŸ”‘ API í‚¤ ì• 10ìë¦¬: {self.api_key[:10]}...")
        
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost:7000",
            "X-Title": "Report Generator"
        }
        
    async def generate_code(self, prompt: str, model_type: ModelType = ModelType.QWEN) -> str:
        """ì½”ë“œ ìƒì„± (ì¼ë°˜ ë²„ì „)"""
        try:
            full_response = ""
            async for chunk in self.generate_code_stream(prompt, model_type):
                full_response += chunk
            return full_response
        except Exception as e:
            logger.error(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def generate_code_stream(self, prompt: str, model_type: ModelType = ModelType.QWEN) -> AsyncIterator[str]:
        """ì½”ë“œ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""
        try:
            model_name = self.qwen_model if model_type == ModelType.QWEN else self.llm_model
            system_prompt = self._get_system_prompt(model_type)
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            payload = {
                "model": model_name,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 4000,
                "stream": True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            }
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream(
                    "POST", 
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload
                ) as response:
                    response.raise_for_status()
                    
                    async for line in response.aiter_lines():
                        if line.strip():
                            if line.startswith("data: "):
                                data_str = line[6:]  # "data: " ì œê±°
                                
                                if data_str.strip() == "[DONE]":
                                    break
                                
                                try:
                                    data = json.loads(data_str)
                                    if "choices" in data and len(data["choices"]) > 0:
                                        delta = data["choices"][0].get("delta", {})
                                        content = delta.get("content", "")
                                        if content:
                                            yield content
                                except json.JSONDecodeError:
                                    continue
                                    
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def generate_completion(
        self, 
        prompt: str, 
        model_type: ModelType = ModelType.LLM,
        max_tokens: int = 100,
        temperature: float = 0.1
    ) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì™„ì„±ì„ ìœ„í•œ ë©”ì„œë“œ (health checkìš©)"""
        
        # health checkìš© ê°„ë‹¨ ì‘ë‹µ
        if prompt == "test":
            return "healthy"
        
        model_name = self.llm_model if model_type == ModelType.LLM else self.qwen_model
        
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False
        }
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=30.0
                )
                response.raise_for_status()
                
                result = response.json()
                
                if "choices" in result and len(result["choices"]) > 0:
                    return result["choices"][0]["message"]["content"]
                else:
                    return "healthy"
                    
        except Exception as e:
            logger.error(f"Completion API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return "unhealthy"
    
    async def fix_code_error(self, error_code: str, error_message: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤."""
        
        fix_prompt = f"""
ë‹¤ìŒ Python ì½”ë“œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ê³  ìˆ˜ì •ëœ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ì˜¤ë¥˜ ì½”ë“œ:
```python
{error_code}
```

ì˜¤ë¥˜ ë©”ì‹œì§€:
{error_message}

ìˆ˜ì •ëœ ì™„ì „í•œ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        return await self.generate_code(
            prompt=fix_prompt,
            model_type=ModelType.LLM
        )
    
    async def enhance_report(self, current_report: str, enhancement_request: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ê°œì„ í•©ë‹ˆë‹¤."""
        
        enhance_prompt = f"""
ë‹¤ìŒ ë°ì´í„° ë¦¬í¬íŠ¸ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:

í˜„ì¬ ë¦¬í¬íŠ¸:
{current_report}

ê°œì„  ìš”ì²­:
{enhancement_request}

ê°œì„ ëœ ì™„ì „í•œ ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        return await self.generate_code(
            prompt=enhance_prompt,
            model_type=ModelType.LLM
        )
    
    def _get_system_prompt(self, model_type: ModelType) -> str:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        
        if model_type == ModelType.QWEN:
            return """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ë° ì›¹ ë¦¬í¬íŠ¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ JSON ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , HTML/JavaScriptë¥¼ ì‚¬ìš©í•˜ì—¬ 
ì¸í„°ë™í‹°ë¸Œí•œ ì‹œê°í™” ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” Python ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ê·œì¹™:
1. ë°˜ë“œì‹œ Python ì½”ë“œì™€ HTML í…œí”Œë¦¿ì„ ìƒì„±í•˜ì„¸ìš”
2. Chart.js, D3.js, Plotly.js ë“±ì˜ ì˜¤í”„ë¼ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì„¸ìš”
3. ìƒì„±ëœ HTMLì€ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
4. ëª¨ë“  ìŠ¤íƒ€ì¼ê³¼ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¡œì»¬ ê²½ë¡œ(/static/)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
5. ë°ì´í„°ëŠ” JSON í˜•íƒœë¡œ ì œê³µë˜ë©°, ì´ë¥¼ ì ì ˆíˆ íŒŒì‹±í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”
6. ì½”ë“œëŠ” ì•ˆì „í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤
7. MCP ë„êµ¬ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì„¸ìš”

ì¶œë ¥ í˜•ì‹:
- Python ì½”ë“œ: ```python ... ```
- HTML ì½”ë“œ: ```html ... ```
- JavaScript ì½”ë“œ (í•„ìš”ì‹œ): ```javascript ... ```
- ì„¤ëª…: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±

ìƒì„±ëœ ë¦¬í¬íŠ¸ëŠ” `/reports/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
"""
        else:  # LLM
            return """
ë‹¹ì‹ ì€ ì½”ë“œ ë¶„ì„ ë° ê°œì„  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì½”ë“œì˜ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ê³ , ë” ë‚˜ì€ í•´ê²°ì±…ì„ ì œì•ˆí•˜ë©°,
ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

ëŠ¥ë ¥:
1. ì½”ë“œ ì˜¤ë¥˜ ë¶„ì„ ë° ìˆ˜ì •
2. ì„±ëŠ¥ ìµœì í™” ì œì•ˆ
3. ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„ 
4. ë°ì´í„° ì‹œê°í™” í–¥ìƒ
5. ë³´ì•ˆ ë° ì•ˆì „ì„± ê²€í† 

í•­ìƒ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    async def health_check(self) -> bool:
        """OpenRouter API ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    headers=self.headers
                )
                return response.status_code == 200
        except Exception as e:
            logger.error(f"OpenRouter API í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False
    
    async def list_available_models(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    headers=self.headers
                )
                response.raise_for_status()
                
                result = response.json()
                return result.get("data", [])
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_info(self) -> Dict[str, str]:
        """í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "qwen_model": self.qwen_model,
            "llm_model": self.llm_model,
            "base_url": self.base_url
        } 