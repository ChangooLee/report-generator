"""
ìŠ¤íŠ¸ë¦¬ë° API ì—”ë“œí¬ì¸íŠ¸
JavaScriptì™€ ì—°ê²°í•˜ì—¬ ì±„íŒ… ê¸°ëŠ¥ì„ ì œê³µ
"""

import asyncio
import json
import logging
from typing import AsyncGenerator, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

logger = logging.getLogger(__name__)

class ChatRequest(BaseModel):
    user_query: str
    session_id: Optional[str] = None
    format: str = "html"

class StreamingCallback:
    """ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.queue = asyncio.Queue()
    
    async def send_status(self, message: str):
        """ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡"""
        await self.queue.put({
            "type": "status",
            "message": message,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_tool_start(self, tool_name: str, server_name: str = ""):
        """ë„êµ¬ ì‹œì‘ ì•Œë¦¼"""
        await self.queue.put({
            "type": "tool_start",
            "tool_name": tool_name,
            "server_name": server_name,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_tool_complete(self, tool_name: str, result: str):
        """ë„êµ¬ ì™„ë£Œ ì•Œë¦¼"""
        await self.queue.put({
            "type": "tool_complete",
            "tool_name": tool_name,
            "result": result[:200] + "..." if len(result) > 200 else result,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_tool_error(self, tool_name: str, error: str):
        """ë„êµ¬ ì˜¤ë¥˜ ì•Œë¦¼"""
        await self.queue.put({
            "type": "tool_error",
            "tool_name": tool_name,
            "error": error,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_llm_start(self, model_name: str):
        """LLM ì‘ë‹µ ì‹œì‘ ì•Œë¦¼"""
        await self.queue.put({
            "type": "llm_start",
            "model": model_name,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_llm_chunk(self, content: str):
        """LLM ì‘ë‹µ ì²­í¬ ì „ì†¡"""
        if content and content.strip():
            await self.queue.put({
                "type": "content",
                "content": content,
                "timestamp": datetime.now().isoformat()
            })
    
    async def send_analysis_step(self, step_name: str, description: str):
        """ë¶„ì„ ë‹¨ê³„ ì•Œë¦¼"""
        await self.queue.put({
            "type": "analysis_step",
            "step": step_name,
            "description": description,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_progress(self, value: int, message: str = ""):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        await self.queue.put({
            "type": "progress",
            "value": min(100, max(0, value)),
            "message": message,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_complete(self, result: Dict[str, Any]):
        """ì™„ë£Œ ì•Œë¦¼"""
        await self.queue.put({
            "type": "complete",
            "result": result,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_error(self, error: str):
        """ì˜¤ë¥˜ ì•Œë¦¼"""
        await self.queue.put({
            "type": "error",
            "error": error,
            "timestamp": datetime.now().isoformat()
        })

def generate_sse_data(event: str, data: Dict[str, Any]) -> str:
    """SSE í˜•ì‹ì˜ ë°ì´í„° ìƒì„±"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def create_streaming_endpoints(app: FastAPI, orchestrator):
    """ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ ìƒì„±"""
    
    @app.post("/chat/stream")
    async def chat_stream(request: ChatRequest):
        """ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸"""
        
        async def stream_generator() -> AsyncGenerator[str, None]:
            streaming_callback = StreamingCallback()
            
            try:
                # ì„¸ì…˜ ID ìƒì„±
                session_id = request.session_id if request.session_id else f"session_{datetime.now().timestamp()}"
                
                # ì‹œì‘ ìƒíƒœ ì „ì†¡
                yield generate_sse_data("message", {"type": "status", "message": "ğŸš€ AI ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤..."})
                await asyncio.sleep(0.2)
                
                # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                logger.info(f"ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: {request.user_query}")
                
                # ë„êµ¬ ì´ˆê¸°í™” ìƒíƒœ
                yield generate_sse_data("message", {"type": "status", "message": "ğŸ” MCP ë„êµ¬ë“¤ì„ ìë™ ë°œê²¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."})
                
                # ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ê°€ì§œ ì‹œë®¬ë ˆì´ì…˜ ì œê±°)
                workflow_task = asyncio.create_task(
                    orchestrator.process_query_with_streaming(request.user_query, session_id, streaming_callback)
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì²˜ë¦¬
                message_count = 0
                while not workflow_task.done() or not streaming_callback.queue.empty():
                    try:
                        # íì—ì„œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 0.1ì´ˆ)
                        message = await asyncio.wait_for(streaming_callback.queue.get(), timeout=0.1)
                        
                        if message.get("type") == "stop":
                            break
                        
                        # UIë¡œ ë©”ì‹œì§€ ì „ì†¡ (ì˜¬ë°”ë¥¸ SSE í˜•ì‹)
                        yield generate_sse_data("message", message)
                        message_count += 1
                        
                        # ì§„í–‰ìƒí™© í‘œì‹œ
                        if message_count % 3 == 0:
                            progress = min(85, message_count * 5)
                            yield generate_sse_data("message", {"type": "progress", "value": progress})
                        
                    except asyncio.TimeoutError:
                        # íƒ€ì„ì•„ì›ƒ - ì›Œí¬í”Œë¡œìš° ì™„ë£Œ í™•ì¸
                        if workflow_task.done():
                            break
                        continue
                
                # ì›Œí¬í”Œë¡œìš° ê²°ê³¼ ëŒ€ê¸°
                result = await workflow_task
                
                # ìµœì¢… ê²°ê³¼ ì „ì†¡
                yield generate_sse_data("message", {"type": "progress", "value": 100, "message": "ì™„ë£Œ"})
                
                if result.get("success"):
                    # HTML ì½”ë“œ ì „ì†¡
                    if result.get("html_content"):
                        yield generate_sse_data("message", {
                            "type": "code",
                            "code": result["html_content"],
                            "filename": f"report_{session_id}.html"
                        })
                    
                    yield generate_sse_data("message", {
                        "type": "complete",
                        "success": True,
                        "analysis": result.get("analysis", "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."),
                        "report_url": result.get("report_url"),
                        "session_id": session_id
                    })
                else:
                    yield generate_sse_data("message", {
                        "type": "error",
                        "message": result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    })
                
            except Exception as e:
                logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                yield generate_sse_data("error", {"message": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"})
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*"
            }
        ) 